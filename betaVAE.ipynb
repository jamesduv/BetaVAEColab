{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variational Autoencoders (VAEs) and Beta-VAE\n",
    "***update with full details***\n",
    "\n",
    "This notebook examines VAEs and beta-VAEs in a variety of contexts, the papers are\n",
    "- Kingma and Welling (2014) Autoencoding Variational Bayes\n",
    "- Higgins et. al (2017)\n",
    "  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "Often times in machine learning the goal is to learn a probabilistic model of some phenomena directly from the data.\n",
    "The true generative process for a dataset may be viewed as a joint distribution between the data $x$ and some set of latent generative factors $z$ which are unobserved, written as $p_{G}(x,z)$. \n",
    "For example, unsupervised representation learning seeks a compact, meaningful representation of the generative factors for use in other analysis, prediction, or decision making tasks. \n",
    "In the framework of variational autoencoders (VAEs), a model for the generative process is expressed as a directed Bayesian network and linked to an observation model, with the unknown conditional probabilities represented using neural networks or some other form of differentiable parametric models. \n",
    "The resulting model is a stochastic function of the inputs, and through a reparameterization the model parameters and latent representation are learned jointly through stochastic-gradient-descent (SGD) based optimization. \n",
    "As with many variational inference methods, VAEs perform the optimization by minimizing the evidence lower bound (ELBO) which seeks to balance data reproduction while maintaining closeness to a prior distribution over the latent factors.\n",
    "\n",
    "This notebook explores using VAEs and Beta-VAEs as a generative model in this context using the classic MNIST dataset as an example. \n",
    "First background material regarding variational inference (VI), VAEs, and Beta-VAEs is covered. \n",
    "Next, we seek to replicate the results from Kingma and Welling using dense and convolutional autoencoder neural networks.\n",
    "Then, Beta-VAE is implemented, a modification to the original VAE approach presented by Higgins et al, is implemented. \n",
    "\n",
    "***Flesh out after complete***"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variational Inference\n",
    "Unsupervised representation learning seeks answers to the question:\n",
    "\n",
    "\"How can we perform inference and learning in directed probabilistic models with continuous latent variables with intractable posterior distributions and large data sets?\"\n",
    "\n",
    "Approximate the true generative process for data $x$ given latent factors $z$ over model parameters $\\theta$ as\n",
    "$$\n",
    "p_{G}(x,z) \\approx p_{\\theta}(x,z) = p_{\\theta}(z) p_{\\theta}(x | z).\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conditional Bayes' rule may be written \n",
    "$$\n",
    "    p(z|x) = \\frac{p(x|z)p(x)}{p(z)} = \\frac{\\mathrm{Likelihood}\\times \\mathrm{Prior}}{\\mathrm{Evidence}}\n",
    "$$\n",
    "\n",
    "Two methods for sampling from the posterior include:\n",
    "\n",
    "- In *Markov Chain Monte Carlo (MCMC)*, construct an ergodic Markov chain with the posterior as the stationary distribution.\n",
    "- In *Variational Inference*, approximate the posterior by positing a family of parametric distributions with parameter $\\mu$, as $q(z|x,\\mu)$ and choose which is `closest' to the target distribution via optimization."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examining variational inference in more detail: the optimization problem may be stated as\n",
    "$$\n",
    "    q^{*}(z|x,\\gamma^{*}) = \\mathrm{argmin} D_{KL}\\big(q(z|x,\\gamma) \\; \\| \\; p(z|x) \\big),\n",
    "$$\n",
    "where $D_{KL}$ is the KL-Divergence, a measure of similarity, written as\n",
    "$$\n",
    "    D_{KL}(q \\| p) = \\mathbb{E}_{\\sim q} \\big[  \\mathrm{log}(\\tfrac{q}{p})     \\big].\n",
    "$$\n",
    "The KL divergence is the *expected difference of information* between $p$ and $q$.\n",
    "See this as the information content of $x$ with respect to $p$ is \n",
    "$$\n",
    "I_{p}(x) = -\\mathrm{log}p(x),\n",
    "$$\n",
    "then the difference of information between $p$ and $q$ is\n",
    "$$\n",
    "\\Delta I(p,q;x) = I_{p}(x) - I_{q}(x) = \\mathrm{log}\\frac{q(x)}{p(x)}.\n",
    "$$\n",
    "The KL divergence is not symmetric, and equals zero when $p=q$.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
